{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_repo = \"./java_repo/csc-java-course-spring-2023-key-value-store-EgorShibaev/\"\n",
    "method1_name = \"org.csc.java.spring2023.KeyValueStoreImplementation.openValueStream\"\n",
    "method2_name = \"org.csc.java.spring2023.KeyValueStoreImplementation.getIndexManager\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook supports only Java repository. \n",
    "\n",
    "For parsing Java code library `javalang` is used. javalang provides a lexer and parser targeting __Java 8__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import javalang\n",
    "import os\n",
    "\n",
    "def extract_code_block(code: str, start_position: int) -> str:\n",
    "    \"\"\"\n",
    "    Extracts the code block starting at the given position according to the opening and closing braces\n",
    "    :param code: Java or Kotlin code\n",
    "    :param start_position: position of the opening brace of the code block\n",
    "    :return: code block and end position of the code block\n",
    "    \"\"\"\n",
    "    open_braces = 0\n",
    "    code_block_start = start_position\n",
    "    code_block_end = -1\n",
    "\n",
    "    for i in range(start_position, len(code)):\n",
    "        if code[i] == \"{\":\n",
    "            open_braces += 1\n",
    "        elif code[i] == \"}\":\n",
    "            open_braces -= 1\n",
    "            if open_braces == 0:\n",
    "                code_block_end = i + 1\n",
    "                break\n",
    "\n",
    "    if code_block_end == -1:\n",
    "        raise ValueError(f\"Code block extraction failed\")\n",
    "\n",
    "    while code[code_block_start] == \"\\n\":\n",
    "        code_block_start += 1\n",
    "\n",
    "    return code[code_block_start:code_block_end], code_block_end\n",
    "\n",
    "\n",
    "\n",
    "def extract_method_code(java_source: str, class_name: str, method_name: str) -> str | None:\n",
    "    \"\"\"\n",
    "    This function extracts the code of the method with the\n",
    "    given name and class from the Java source code\n",
    "    :param java_source: Java source code\n",
    "    :param class_name: name of the class\n",
    "    :param method_name: name of the method\n",
    "    :return: code of the method\n",
    "    \"\"\"\n",
    "    tree = javalang.parse.parse(java_source)\n",
    "\n",
    "    for path, node in tree.filter(javalang.tree.MethodDeclaration):\n",
    "        if class_name == path[1][0].name and method_name == node.name:\n",
    "            start_line = node.position.line - 1\n",
    "            start_position = sum(len(line) + 1 for line in java_source.splitlines()[:start_line]) \\\n",
    "                             + node.position.column - 1\n",
    "            return extract_code_block(java_source, start_position)[0]\n",
    "\n",
    "    return None\n",
    "\n",
    "def extract_method_code_by_repo_path_and_fqname(path_to_repository: str, fully_qualified_name: str) -> str | None:\n",
    "    \"\"\"\n",
    "    This function extracts the code of the method with the\n",
    "    given fully qualified name from the repository\n",
    "    :param path_to_repository: path to the repository\n",
    "    :param fully_qualified_name: fully qualified name of the method\n",
    "    :return: code of the method\n",
    "    \"\"\"\n",
    "\n",
    "    if fully_qualified_name.count(\".\") >= 2:\n",
    "        package, class_name, method_name = fully_qualified_name.rsplit(\".\", 2)\n",
    "    else:\n",
    "        class_name, method_name = fully_qualified_name.rsplit(\".\", 1)\n",
    "        package = \"\"\n",
    "\n",
    "    package_path = package.replace(\".\", os.path.sep)\n",
    "    file_path = os.path.join(path_to_repository, \"src\", \"main\", \"java\", package_path, f\"{class_name}.java\")\n",
    "\n",
    "    with open(file_path, \"r\") as java_file:\n",
    "        java_source = java_file.read()\n",
    "\n",
    "    # Extract the method code\n",
    "    return extract_method_code(java_source, class_name, method_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "method1_code = extract_method_code_by_repo_path_and_fqname(path_to_repo, method1_name)\n",
    "method2_code = extract_method_code_by_repo_path_and_fqname(path_to_repo, method2_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'InputStream openValueStream(byte[] key) throws IOException {\\n    Objects.requireNonNull(key);\\n    if (!contains(key)) {\\n      throw new IOException(\"No such key in store\");\\n    }\\n    check_closed();\\n\\n\\n    var blocks = indexManager.getFileBlocksLocations(key);\\n    var stream = InputStream.nullInputStream();\\n\\n    for (var block : blocks) {\\n      stream = new SequenceInputStream(stream, valueStoreManager.openBlockStream(block));\\n    }\\n    return stream;\\n  }'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method1_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'IndexManager getIndexManager() {\\n    check_closed();\\n\\n    return indexManager;\\n  }'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "method2_code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will try 3 different models for this task: two models which are trained on the Java code and one model which is trained on the text datasets. The first two models are `microsoft/CodeGPT-small-java-adaptedGPT2` and `neulab/codebert-java` from Huggingface and the third model is `all-MiniLM-L6-v2` from Sentence Transformer library.\n",
    "\n",
    "For each model, I find embeddings for each method in pairs and consider the cosine distance between embeddings as a similarity measure.\n",
    "\n",
    "I expect that models trained on Java code will extract meaningful features from the code and methods with similar semantics will be closer to each other.\n",
    "\n",
    "The model from Sentence Transformer is trained for the task of semantic textual similarity, so, it is probable that it will be suitable for code similarity tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalization\n",
    "\n",
    "The first two code-trained models are not specifically trained for the semantic similarity task. Consequently, the manifold of their embeddings may be located far from the origin. This can lead to a situation where my approach consistently considers methods to be similar (e. g. CodeBERT without normalization always output cosine similairty > 0.9):\n",
    "\n",
    "![Normalization](./imgs/normalization.jpg)\n",
    "\n",
    "To address this issue, I apply batch normalization to each method's embedding. To initialize the batch normalization statistics, I utilize a dataset from Huggingface containing Java methods. I only use a small number of methods for this purpose, as it is sufficient for obtaining the necessary statistics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "\n",
    "batch_size = 4\n",
    "num_batches_to_iterate = 100 # number of batches to load and process\n",
    "\n",
    "java_methods_test_dataset = load_dataset(\"anjandash/java-8m-methods-v2\", split=\"test\", streaming=True)\n",
    "dataloader = DataLoader(java_methods_test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "def run_model_through_dataloader(model, tokenizer, batch_norm, dataloader):\n",
    "    all_embs = []\n",
    "    for batch_idx, batch_data in tqdm(enumerate(dataloader), total=num_batches_to_iterate, desc='iterating over batches to init batch norm'):\n",
    "        if batch_idx >= num_batches_to_iterate:\n",
    "            break\n",
    "        tokenized_batch = tokenizer(batch_data['text'], padding=True, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        embs = model(**tokenized_batch).last_hidden_state.mean(dim=1)\n",
    "        all_embs.append(embs)\n",
    "    all_embs = torch.cat(all_embs)\n",
    "    batch_norm(all_embs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# classes-wrappers for models from different sources\n",
    "class SentenceTransformerModel:\n",
    "\n",
    "    def __init__(self, model_name):\n",
    "        self.model = SentenceTransformer(model_name)\n",
    "    \n",
    "    def encode(self, method_code):\n",
    "        return self.model.encode(method_code, convert_to_tensor=True).view(1, -1)\n",
    "\n",
    "class HuggingfaceModel:\n",
    "\n",
    "    def __init__(self, model_name, output_size=768):\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModel.from_pretrained(model_name)\n",
    "\n",
    "        self.batchnorm = torch.nn.BatchNorm1d(output_size, momentum=1.0, affine=False)\n",
    "        # intialize batchnorm\n",
    "        self.batchnorm.train()\n",
    "        with torch.no_grad():\n",
    "            run_model_through_dataloader(self.model, self.tokenizer, self.batchnorm, dataloader)\n",
    "        self.batchnorm.eval()\n",
    "    \n",
    "    def encode(self, method_code):\n",
    "        inputs = self.tokenizer(method_code, return_tensors=\"pt\")\n",
    "        outputs = self.model(**inputs)\n",
    "        return self.batchnorm(outputs.last_hidden_state.mean(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82a8c0bce46a46d08a33da497e4f0bb6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "iterating over batches to init batch norm:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of RobertaModel were not initialized from the model checkpoint at neulab/codebert-java and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1aa66fb1677e46688003fc84d6f3b54d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "iterating over batches to init batch norm:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "models = {\n",
    "    \"CodeGPT\": HuggingfaceModel(\"microsoft/CodeGPT-small-java-adaptedGPT2\"),\n",
    "    \"CodeBERT\": HuggingfaceModel(\"neulab/codebert-java\"),\n",
    "    \"MiniLM\": SentenceTransformerModel(\"all-MiniLM-L6-v2\"),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def get_similarity(model_name, method1_code, method2_code):\n",
    "\n",
    "    with torch.no_grad():\n",
    "        embedding_1 = models[model_name].encode(method1_code)\n",
    "        embedding_2 = models[model_name].encode(method2_code)\n",
    "\n",
    "    similarity = torch.nn.functional.cosine_similarity(embedding_1, embedding_2)\n",
    "\n",
    "    return similarity.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity by CodeGPT: 0.629\n",
      "Similarity by CodeBERT: 0.495\n",
      "Similarity by MiniLM: 0.368\n"
     ]
    }
   ],
   "source": [
    "for model_name in models:\n",
    "    similarity = get_similarity(model_name, method1_code, method2_code)\n",
    "    print(f\"Similarity by {model_name}: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some comparison of models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_models(code1, code2):\n",
    "    for model_name in models:\n",
    "        similarity = get_similarity(model_name, code1, code2)\n",
    "        print(f\"Similarity by {model_name}: {similarity:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity by CodeGPT: 0.516\n",
      "Similarity by CodeBERT: 0.369\n",
      "Similarity by MiniLM: 0.014\n"
     ]
    }
   ],
   "source": [
    "compare_models(\"\"\"\n",
    "public int factorial(int n) {\n",
    "    if (n == 0) {\n",
    "        return 1;\n",
    "    } else {\n",
    "        return n * factorial(n - 1);\n",
    "    }\n",
    "}\n",
    "\"\"\",\"\"\"\n",
    "public String[] parseCSVLine(String csvLine) {\n",
    "    return csvLine.split(\",\");\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity by CodeGPT: 0.611\n",
      "Similarity by CodeBERT: 0.455\n",
      "Similarity by MiniLM: 0.385\n"
     ]
    }
   ],
   "source": [
    "compare_models(\"\"\"\n",
    "public int calculateSquare(int num) {\n",
    "    return num * num;\n",
    "}\n",
    "\"\"\",\"\"\"\n",
    "public boolean isNumeric(String str) {\n",
    "    try {\n",
    "        Double.parseDouble(str);\n",
    "        return true;\n",
    "    } catch (NumberFormatException e) {\n",
    "        return false;\n",
    "    }\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity by CodeGPT: 0.480\n",
      "Similarity by CodeBERT: 0.311\n",
      "Similarity by MiniLM: 0.036\n"
     ]
    }
   ],
   "source": [
    "compare_models(\"\"\"\n",
    "public int findMax(int[] arr) {\n",
    "    int max = arr[0];\n",
    "    for (int i = 1; i < arr.length; i++) {\n",
    "        if (arr[i] > max) {\n",
    "            max = arr[i];\n",
    "        }\n",
    "    }\n",
    "    return max;\n",
    "}\n",
    "\"\"\",\"\"\"\n",
    "public String toUpperCase(String input) {\n",
    "    return input.toUpperCase();\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I consider methods below similar (same algorithm used), so, I expect that the similarity score for them will be high.\n",
    "\n",
    " CodeGPT and CodeBERT output high score, but Sentence Transformer model output much lower score, so, it is not able to see similarity between these methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity by CodeGPT: 0.970\n",
      "Similarity by CodeBERT: 0.982\n",
      "Similarity by MiniLM: 0.601\n"
     ]
    }
   ],
   "source": [
    "compare_models(\"\"\"\n",
    "public static int sum(List<Integer> numbers) {\n",
    "        int sum = 0;\n",
    "        for (int num : numbers) {\n",
    "            sum += num;\n",
    "        }\n",
    "        return sum;\n",
    "}\n",
    "\"\"\",\"\"\"\n",
    "public static int product(List<Integer> numbers) {\n",
    "        int product = 1;\n",
    "        for (int num : numbers) {\n",
    "            product *= num;\n",
    "        }\n",
    "        return product;\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity by CodeGPT: 0.935\n",
      "Similarity by CodeBERT: 0.830\n",
      "Similarity by MiniLM: 0.753\n"
     ]
    }
   ],
   "source": [
    "compare_models(\"\"\"\n",
    "public static int sqr(int a) {\n",
    "    return a * a;\n",
    "}\n",
    "\"\"\",\"\"\"\n",
    "public static long sqr(int a) {\n",
    "    return Math.pow(a, 2);\n",
    "}\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Resume\n",
    "\n",
    "I compared 3 models on pairs of java methods. \n",
    "- all-MiniLM-L6-v2 from Sentence Transformers library is not the best choice for this task because it was trained on large amounts of text data to generate sentence embeddings that capture the semantic meaning of the __text__, so, it is not suitable for code similarity task.\n",
    "- CodeGPT performs similar to CodeBERT. So, I will use CodeGPT for the second task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook task1.ipynb to html\n",
      "[NbConvertApp] Writing 655690 bytes to task1.html\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to html task1.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
